{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "\n",
    "def bits_to_gb(bits):\n",
    "    return bits / (8 * 1024**3)\n",
    "\n",
    "\n",
    "def calculate_train_vram_requirements(\n",
    "        batch_size, seq_len, params, precision, num_layers, num_attn_heads, hidden_size, **ignored\n",
    "):\n",
    "    \"\"\"\n",
    "    full train, not lora\n",
    "    source: https://arxiv.org/pdf/2205.05198.pdf (section 4.1)\n",
    "    credit: https://medium.com/@siddheshgunjal82/understanding-vram-requirements-to-train-inference-with-large-language-models-llms-a3edd0f09d9f\n",
    "    \"\"\"\n",
    "    # Calculate activations using the provided formula\n",
    "    activations = (\n",
    "        num_layers * (5/2) * num_attn_heads * batch_size * seq_len**2\n",
    "                   + 17 * batch_size * hidden_size * seq_len\n",
    "    )\n",
    "\n",
    "    # Calculate VRAM using the provided formula\n",
    "    vram_bits = precision * (activations + params)\n",
    "\n",
    "    # Convert VRAM from bits to Gigabytes\n",
    "    return bits_to_gb(vram_bits)\n",
    "\n",
    "\n",
    "def calculate_inference_vram_requirements(\n",
    "        batch_size, seq_len, params, precision, num_layers, hidden_size,\n",
    "        num_attn_heads, num_kv_heads, gqa=True\n",
    "):\n",
    "    \"\"\"\n",
    "    source 1: https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n",
    "    source 2: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\n",
    "    - same as source 1, but with the introduction a factor (n_heads / n_kv_heads) specific to GQA\n",
    "      - \"GQA helps with keeping the KV cache size down by sharing Keys/Values\"\n",
    "    - defaulting to calculated models using GQA since Mistral, Yi, and Llama 2 use it\n",
    "    \"\"\"\n",
    "    kv_cache = batch_size * seq_len * 2 * num_layers * hidden_size\n",
    "    if gqa:\n",
    "        kv_cache *= num_kv_heads / num_attn_heads\n",
    "\n",
    "    vram_bits = precision * (kv_cache + params)\n",
    "\n",
    "    return bits_to_gb(vram_bits)\n",
    "\n",
    "def get_model_params(model_uri):\n",
    "    hf_hub_download(repo_id=model_uri, filename=\"config.json\", local_dir=\".\")\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        model_params = json.load(f)\n",
    "    return model_params\n",
    "\n",
    "def print_table(model_uri, bparams, batch_size=1, precisions=None, mode=\"infer\"):\n",
    "    precisions = precisions or [4, 6, 8, 16]\n",
    "\n",
    "    model_params = get_model_params(model_uri)\n",
    "\n",
    "    seq_lens = (\n",
    "        [2**i for i in range(8, 20) if 2**i< model_params[\"max_position_embeddings\"]]\n",
    "        + [model_params[\"max_position_embeddings\"]]\n",
    "    )\n",
    "\n",
    "    calc_params = {\n",
    "        \"num_layers\": model_params[\"num_hidden_layers\"],\n",
    "        \"hidden_size\": model_params[\"hidden_size\"],\n",
    "        \"num_attn_heads\": model_params[\"num_attention_heads\"],\n",
    "        \"num_kv_heads\": model_params[\"num_key_value_heads\"],\n",
    "    }\n",
    "\n",
    "    if mode == \"infer\":\n",
    "        vram_calculator = calculate_inference_vram_requirements\n",
    "    elif mode == \"train\":\n",
    "        vram_calculator = calculate_train_vram_requirements\n",
    "    elif mode == \"train_lora\":\n",
    "        raise NotImplemented\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    column_width = 10\n",
    "\n",
    "    # Print the header of the table with precisions\n",
    "    header = f\"{'SL / BP':>{column_width}}\" + \"\".join([f\" | {p:^10}\" for p in precisions])\n",
    "    results = [\n",
    "        f\"Model: {model_uri}\",\n",
    "        f\"Params: {bparams}B\",\n",
    "        f\"Batch Size: {batch_size}\",\n",
    "        f\"Mode: {mode}\",\n",
    "        \"\",\n",
    "        \"Sequence Length vs Bit Precision - Memory Requirements\"\n",
    "    ]\n",
    "    results.append(header)\n",
    "    results.append(\"-\" * len(header))\n",
    "\n",
    "    # Iterate over each seq_len and calculate VRAM for each precision\n",
    "    for seq_len in seq_lens:\n",
    "        seq_len_label = f\"{seq_len:>{column_width}}\"\n",
    "        if seq_len == max(seq_lens):\n",
    "            seq_len_label = \"*\" + seq_len_label[1:]\n",
    "        row_data = [seq_len_label]\n",
    "        for precision in precisions:\n",
    "            vram_required = vram_calculator(\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "                precision=precision,\n",
    "                params=bparams * 1e9,\n",
    "                **calc_params  # Unpack additional parameters if provided\n",
    "            )\n",
    "            row_data.append(f\"{vram_required:8.1f}GB\")  # Format with 1 decimal point\n",
    "\n",
    "        # Print each row of the table\n",
    "        results.append(\" | \".join(row_data))\n",
    "\n",
    "    results += [\"\", \"* Model Max Context Size\"]\n",
    "\n",
    "    print(\"    \" + \"\\n    \".join(results))\n",
    "\n",
    "    # save everything to a file\n",
    "    if f\"{model_uri.replace('/', '-')}-{mode}.txt\" in os.listdir():\n",
    "        # append to existing file\n",
    "        with open(f\"{model_uri.replace('/', '-')}-{mode}.txt\", \"a\") as f:\n",
    "            f.write(\"\\n\\n\" + \"\\n\".join(results))\n",
    "    else:\n",
    "        # create new file\n",
    "        with open(f\"{model_uri.replace('/', '-')}-{mode}.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model: meta-llama/Meta-Llama-3-8B\n",
      "    Params: 8.03B\n",
      "    Batch Size: 1\n",
      "    Mode: train\n",
      "    \n",
      "    Sequence Length vs Bit Precision - Memory Requirements\n",
      "       SL / BP |     4      |     6      |     8      |     16    \n",
      "    --------------------------------------------------------------\n",
      "           256 |      3.8GB |      5.7GB |      7.7GB |     15.3GB\n",
      "           512 |      4.1GB |      6.1GB |      8.1GB |     16.3GB\n",
      "          1024 |      5.0GB |      7.5GB |     10.0GB |     20.1GB\n",
      "          2048 |      8.8GB |     13.2GB |     17.6GB |     35.2GB\n",
      "          4096 |     23.9GB |     35.8GB |     47.7GB |     95.5GB\n",
      "    *     8192 |     84.0GB |    126.0GB |    168.0GB |    336.0GB\n",
      "    \n",
      "    * Model Max Context Size\n"
     ]
    }
   ],
   "source": [
    "print_table(\"meta-llama/Meta-Llama-3-8B\", batch_size=1, bparams=8.03, mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
