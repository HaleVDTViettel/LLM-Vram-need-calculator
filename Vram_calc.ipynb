{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "\n",
    "def bits_to_gb(bits):\n",
    "    return bits / (8 * 1024**3)\n",
    "\n",
    "\n",
    "def calculate_train_vram_requirements(\n",
    "        batch_size, seq_len, params, precision, num_layers, num_attn_heads, hidden_size, **ignored\n",
    "):\n",
    "    \"\"\"\n",
    "    full train, not lora\n",
    "    source: https://arxiv.org/pdf/2205.05198.pdf (section 4.1)\n",
    "    credit: https://medium.com/@siddheshgunjal82/understanding-vram-requirements-to-train-inference-with-large-language-models-llms-a3edd0f09d9f\n",
    "    \"\"\"\n",
    "    # Calculate activations using the provided formula\n",
    "    activations = (\n",
    "        num_layers * (5/2) * num_attn_heads * batch_size * seq_len**2\n",
    "                   + 17 * batch_size * hidden_size * seq_len\n",
    "    )\n",
    "\n",
    "    # Calculate VRAM using the provided formula\n",
    "    vram_bits = precision * (activations + params)\n",
    "\n",
    "    # Convert VRAM from bits to Gigabytes\n",
    "    return bits_to_gb(vram_bits)\n",
    "\n",
    "\n",
    "def calculate_inference_vram_requirements(\n",
    "        batch_size, seq_len, params, precision, num_layers, hidden_size,\n",
    "        num_attn_heads, num_kv_heads, gqa=True\n",
    "):\n",
    "    \"\"\"\n",
    "    source 1: https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\n",
    "    source 2: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\n",
    "    - same as source 1, but with the introduction a factor (n_heads / n_kv_heads) specific to GQA\n",
    "      - \"GQA helps with keeping the KV cache size down by sharing Keys/Values\"\n",
    "    - defaulting to calculated models using GQA since Mistral, Yi, and Llama 2 use it\n",
    "    \"\"\"\n",
    "    kv_cache = batch_size * seq_len * 2 * num_layers * hidden_size\n",
    "    if gqa:\n",
    "        kv_cache *= num_kv_heads / num_attn_heads\n",
    "\n",
    "    vram_bits = precision * (kv_cache + params)\n",
    "\n",
    "    return bits_to_gb(vram_bits)\n",
    "\n",
    "def get_model_params(model_uri):\n",
    "    hf_hub_download(repo_id=model_uri, filename=\"config.json\", local_dir=\".\")\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        model_params = json.load(f)\n",
    "    return model_params\n",
    "\n",
    "def print_table(model_uri, bparams, batch_size=1, precisions=None, mode=\"infer\"):\n",
    "    precisions = precisions or [4, 6, 8, 16]\n",
    "\n",
    "    model_params = get_model_params(model_uri)\n",
    "\n",
    "    seq_lens = (\n",
    "        [2**i for i in range(8, 20) if 2**i< model_params[\"max_position_embeddings\"]]\n",
    "        + [model_params[\"max_position_embeddings\"]]\n",
    "    )\n",
    "\n",
    "    calc_params = {\n",
    "        \"num_layers\": model_params[\"num_hidden_layers\"],\n",
    "        \"hidden_size\": model_params[\"hidden_size\"],\n",
    "        \"num_attn_heads\": model_params[\"num_attention_heads\"],\n",
    "        \"num_kv_heads\": model_params[\"num_key_value_heads\"],\n",
    "    }\n",
    "\n",
    "    if mode == \"infer\":\n",
    "        vram_calculator = calculate_inference_vram_requirements\n",
    "    elif mode == \"train\":\n",
    "        vram_calculator = calculate_train_vram_requirements\n",
    "    elif mode == \"train_lora\":\n",
    "        raise NotImplemented\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    column_width = 10\n",
    "\n",
    "    # Print the header of the table with precisions\n",
    "    header = f\"{'SL / BP':>{column_width}}\" + \"\".join([f\" | {p:^10}\" for p in precisions])\n",
    "    results = [\n",
    "        f\"Model: {model_uri}\",\n",
    "        f\"Params: {bparams}B\",\n",
    "        f\"Batch Size: {batch_size}\",\n",
    "        f\"Mode: {mode}\",\n",
    "        \"\",\n",
    "        \"Sequence Length vs Bit Precision - Memory Requirements\"\n",
    "    ]\n",
    "    results.append(header)\n",
    "    results.append(\"-\" * len(header))\n",
    "\n",
    "    # Iterate over each seq_len and calculate VRAM for each precision\n",
    "    for seq_len in seq_lens:\n",
    "        seq_len_label = f\"{seq_len:>{column_width}}\"\n",
    "        if seq_len == max(seq_lens):\n",
    "            seq_len_label = \"*\" + seq_len_label[1:]\n",
    "        row_data = [seq_len_label]\n",
    "        for precision in precisions:\n",
    "            vram_required = vram_calculator(\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "                precision=precision,\n",
    "                params=bparams * 1e9,\n",
    "                **calc_params  # Unpack additional parameters if provided\n",
    "            )\n",
    "            row_data.append(f\"{vram_required:8.1f}GB\")  # Format with 1 decimal point\n",
    "\n",
    "        # Print each row of the table\n",
    "        results.append(\" | \".join(row_data))\n",
    "\n",
    "    results += [\"\", \"* Model Max Context Size\"]\n",
    "\n",
    "    print(\"    \" + \"\\n    \".join(results))\n",
    "\n",
    "    # save everything to a file\n",
    "    with open(f\"{model_uri.replace('/', '-')}-{mode}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df3557f7965459fa6e445ea0a1138cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model: alpindale/WizardLM-2-8x22B\n",
      "    Params: 141B\n",
      "    Batch Size: 1\n",
      "    Mode: train\n",
      "    \n",
      "    Sequence Length vs Bit Precision - Memory Requirements\n",
      "       SL / BP |     4      |     6      |     8      |     16    \n",
      "    --------------------------------------------------------------\n",
      "           256 |     65.9GB |     98.8GB |    131.8GB |    263.5GB\n",
      "           512 |     66.5GB |     99.8GB |    133.0GB |    266.0GB\n",
      "          1024 |     69.0GB |    103.5GB |    138.0GB |    276.0GB\n",
      "          2048 |     78.9GB |    118.3GB |    157.8GB |    315.5GB\n",
      "          4096 |    118.4GB |    177.5GB |    236.7GB |    473.4GB\n",
      "          8192 |    276.1GB |    414.1GB |    552.1GB |   1104.2GB\n",
      "         16384 |    906.5GB |   1359.7GB |   1812.9GB |   3625.8GB\n",
      "         32768 |   3427.3GB |   5140.9GB |   6854.5GB |  13709.0GB\n",
      "    *    65536 |  13508.8GB |  20263.3GB |  27017.7GB |  54035.4GB\n",
      "    \n",
      "    * Model Max Context Size\n"
     ]
    }
   ],
   "source": [
    "print_table(\"alpindale/WizardLM-2-8x22B\", bparams=141, mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
